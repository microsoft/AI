{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use a subset of [Stack Exchange network](https://archive.org/details/stackexchange) question data which includes original questions tagged as 'JavaScript', their duplicate questions and their answers. Here, we provide the steps to prepare the data to use in model development for training a model that will match a new question with an existing original question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utilities import read_csv_gz, clean_text, round_sample_strat, random_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define some parameters that will be used in the data cleaning as well as train and test set preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the test set\n",
    "test_size = 0.21\n",
    "# The minimum length of clean text\n",
    "min_text = 150\n",
    "# The minimum number of duplicates per question\n",
    "min_dupes = 12\n",
    "# The maximum number of duplicate matches\n",
    "match = 20\n",
    "# The output files path\n",
    "outputs_path = os.path.join('.', 'data_folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the questions, duplicate questions and answers and load the datasets into pandas dataframes using the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to original questions, duplicate questions, and answers.\n",
    "data_url = \"https://bostondata.blob.core.windows.net/stackoverflow/{}\"\n",
    "questions_url = data_url.format(\"orig-q.tsv.gz\")\n",
    "dupes_url = data_url.format(\"dup-q.tsv.gz\")\n",
    "answers_url = data_url.format(\"ans.tsv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "questions = read_csv_gz(questions_url, names=('Id', 'AnswerId', 'Text0', 'CreationDate'))\n",
    "dupes = read_csv_gz(dupes_url, names=('Id', 'AnswerId', 'Text0', 'CreationDate'))\n",
    "answers = read_csv_gz(answers_url, names=('Id', 'Text0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the dataframes. Notice that questions and duplicates have \"AnswerID\" column that would help match with the index of answers dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first original question's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the duplicates for that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes[dupes.AnswerId == questions.iloc[0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the answer to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.at[questions.iloc[0,0],'Text0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the helper functions to clean questions, duplicates and answers from unwanted text such as code, html tags and links. Notice that we add a new column 'Text' to each dataframe for clean text in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all text, and keep only data with some clean text.\n",
    "for df in (questions, dupes, answers):\n",
    "    df[\"Text\"] = df.Text0.apply(clean_text).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions[questions.Text.str.len() > 0]\n",
    "answers = answers[answers.Text.str.len() > 0]\n",
    "dupes = dupes[dupes.Text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the first original question and cleaned version as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original question.\n",
    "questions.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning.\n",
    "questions.iloc[0,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it turns out that some duplicate questions were also in original questions. Also, some original questions and some duplicate questions were duplicated in the datasets. In the following, we remove them from the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, remove dupes that are questions, then remove duplicated questions and dupes.\n",
    "dupes = dupes[~dupes.index.isin(questions.index)]\n",
    "questions = questions[~questions.index.duplicated(keep='first')]\n",
    "dupes = dupes[~dupes.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make sure we keep questions with answers and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only questions with answers and dupes, answers to questions, and dupes of questions.\n",
    "questions = questions[\n",
    "    questions.AnswerId.isin(answers.index) & questions.AnswerId.isin(dupes.AnswerId)\n",
    "]\n",
    "answers = answers[answers.index.isin(questions.AnswerId)]\n",
    "dupes = dupes[dupes.AnswerId.isin(questions.AnswerId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity.\n",
    "assert questions.AnswerId.isin(answers.index).all()\n",
    "assert answers.index.isin(questions.AnswerId).all()\n",
    "assert questions.AnswerId.isin(dupes.AnswerId).all()\n",
    "assert dupes.AnswerId.isin(questions.AnswerId).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some statistics on the data. Notice that some questions have very low number of duplicates while others may have a large number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report on the data.\n",
    "print(\"Text statistics:\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            questions.Text.str.len().describe().rename(\"questions\"),\n",
    "            answers.Text.str.len().describe().rename(\"answers\"),\n",
    "            dupes.Text.str.len().describe().rename(\"dupes\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(\"\\nDuplication statistics:\")\n",
    "print(pd.DataFrame([dupes.AnswerId.value_counts().describe().rename(\"duplications\")]))\n",
    "print(\n",
    "    \"\\nLargest class: {:.2%}\".format(\n",
    "        dupes.AnswerId.value_counts().max() / dupes.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reset all indexes to use them as columns in the rest of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset each dataframe's index.\n",
    "questions.reset_index(inplace=True)\n",
    "answers.reset_index(inplace=True)\n",
    "dupes.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the questions and duplicates to have at least min_text number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the minimum text length to questions and dupes.\n",
    "questions = questions[questions.Text.str.len() >= min_text]\n",
    "dupes = dupes[dupes.Text.str.len() >= min_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only questions with dupes, and dupes of questions.\n",
    "label_column = \"AnswerId\"\n",
    "questions = questions[questions[label_column].isin(dupes[label_column])]\n",
    "dupes = dupes[dupes[label_column].isin(questions[label_column])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we remove questions and their duplicates that are less than min_dupes parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the questions to those with a minimum number of dupes.\n",
    "answerid_count = dupes.groupby(label_column)[label_column].count()\n",
    "answerid_min = answerid_count.index[answerid_count >= min_dupes]\n",
    "questions = questions[questions[label_column].isin(answerid_min)]\n",
    "dupes = dupes[dupes[label_column].isin(answerid_min)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Verify data integrity.\n",
    "assert questions[label_column].isin(dupes[label_column]).all()\n",
    "assert dupes[label_column].isin(questions[label_column]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some statistics on the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report on the data.\n",
    "print(\"Restrictions: min_text={}, min_dupes={}\".format(min_text, min_dupes))\n",
    "print(\"Restricted text statistics:\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            questions.Text.str.len().describe().rename(\"questions\"),\n",
    "            dupes.Text.str.len().describe().rename(\"dupes\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(\"\\nRestricted duplication statistics:\")\n",
    "print(\n",
    "    pd.DataFrame([dupes[label_column].value_counts().describe().rename(\"duplications\")])\n",
    ")\n",
    "print(\n",
    "    \"\\nRestricted largest class: {:.2%}\".format(\n",
    "        dupes[label_column].value_counts().max() / dupes.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we prepare train and test sets. For training a binary classification model, we will need to construct match and non-match pairs from duplicates and their questions. Finding matching pairs can be accomplished by joining each duplicate with its question. However, non-match examples need to be constructed randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, to make sure we train and test the performance of the model on each question, we will need to have examples of match and non-match pairs for each question both in train and test sets. In order to achieve that, we split the duplicates in a stratified manner into train and test sets making sure at least 1 or more duplicates per question is in the test set depending on test_size parameter and number of duplicates per each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dupes into train and test ensuring at least one of each label class is in test.\n",
    "dupes_test = round_sample_strat(dupes, dupes[label_column], frac=test_size)\n",
    "dupes_train = dupes[~dupes.Id.isin(dupes_test.Id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (dupes_test[label_column].unique().shape[0] == dupes[label_column].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relevant columns for text pairs data.\n",
    "balanced_pairs_columns = ['Id_x', 'AnswerId_x', 'Text_x', 'Id_y', 'Text_y', 'AnswerId_y', 'Label', 'n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pair each training duplicate in train set with its matching question and N-1 random questions using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AnswerId to pair each training dupe with its matching question and also with N-1 questions not its match.\n",
    "%time balanced_pairs_train = random_merge(dupes_train, questions, N=match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling is done such that matching pairs are labeled as 1 and non-match pairs are labeled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label records by matching AnswerIds.\n",
    "balanced_pairs_train[\"Label\"] = (\n",
    "    balanced_pairs_train.AnswerId_x == balanced_pairs_train.AnswerId_y\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant data.\n",
    "balanced_pairs_train = balanced_pairs_train[balanced_pairs_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_pairs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by dupe ID and Label.\n",
    "balanced_pairs_train.sort_values(by=['Id_x', 'Label'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In testing set, we match each duplicate with all the original questions and label them same way as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AnswerId to pair each testing dupe with all questions.\n",
    "%time balanced_pairs_test = random_merge(dupes_test, questions, N=questions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label records by matching AnswerIds.\n",
    "balanced_pairs_test[\"Label\"] = (\n",
    "    balanced_pairs_test.AnswerId_x == balanced_pairs_test.AnswerId_y\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant data.\n",
    "balanced_pairs_test = balanced_pairs_test[balanced_pairs_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_pairs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by dupe ID and Label.\n",
    "balanced_pairs_test.sort_values(\n",
    "    by=[\"Id_x\", \"Label\"], ascending=[True, False], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we report the final train and test sets and save as text files to be used by modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report on the datasets.\n",
    "print(\n",
    "    \"balanced_pairs_train: {:,} rows with {:.2%} matches\".format(\n",
    "        balanced_pairs_train.shape[0], balanced_pairs_train.Label.mean()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"balanced_pairs_test: {:,} rows with {:.2%} matches\".format(\n",
    "        balanced_pairs_test.shape[0], balanced_pairs_test.Label.mean()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "\n",
    "# Save the data.\n",
    "balanced_pairs_train_path = os.path.join(outputs_path, \"balanced_pairs_train.tsv\")\n",
    "print(\n",
    "    \"Writing {:,} to {}\".format(\n",
    "        balanced_pairs_train.shape[0], balanced_pairs_train_path\n",
    "    )\n",
    ")\n",
    "balanced_pairs_train.to_csv(\n",
    "    balanced_pairs_train_path, sep=\"\\t\", header=True, index=False\n",
    ")\n",
    "\n",
    "balanced_pairs_test_path = os.path.join(outputs_path, \"balanced_pairs_test.tsv\")\n",
    "print(\n",
    "    \"Writing {:,} to {}\".format(balanced_pairs_test.shape[0], balanced_pairs_test_path)\n",
    ")\n",
    "balanced_pairs_test.to_csv(balanced_pairs_test_path, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "# Save original questions to be used for scoring later.\n",
    "questions_path = os.path.join(outputs_path, \"questions.tsv\")\n",
    "print(\"Writing {:,} to {}\".format(questions.shape[0], questions_path))\n",
    "questions.to_csv(questions_path, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "# Save the test duplicate questions to be used with the scoring function.\n",
    "dupes_test_path = os.path.join(outputs_path, \"dupes_test.tsv\")\n",
    "print(\"Writing {:,} to {}\".format(dupes_test.shape[0], dupes_test_path))\n",
    "dupes_test.to_csv(dupes_test_path, sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move on to [train on local](02_TrainOnLocal.ipynb) notebook to train our model using Azure Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployAML]",
   "language": "python",
   "name": "conda-env-MLAKSDeployAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
